{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# PCA\n\nПусть имеется $n$ числовых признаков $f_j(x), j=1, \\ldots, n$. Объекты обучающей выборки будем отождествлять с их признаковыми описаниями: $x_i \\equiv\\left(f_1\\left(x_i\\right), \\ldots, f_n\\left(x_i\\right)\\right), i=1, \\ldots, l$. Рассмотрим матрицу $F$, строки которой соответствуют признаковым описаниям обучающих объектов:\n$$\nF_{l \\times n}=\\left(\\begin{array}{ccc}\nf_1\\left(x_1\\right) & \\ldots & f_n\\left(x_1\\right) \\\\\n\\ldots & \\ldots & \\ldots \\\\\nf_1\\left(x_l\\right) & \\ldots & f_n\\left(x_l\\right)\n\\end{array}\\right)=\\left(\\begin{array}{c}\nx_1 \\\\\n\\ldots \\\\\nx_l\n\\end{array}\\right) .\n$$\nОбозначим через $z_i=\\left(g_1\\left(x_i\\right), \\ldots, g_m\\left(x_i\\right)\\right)$ признаковые описания тех же объектов в новом пространстве $Z=\\mathbb{R}^m$ меньшей размерности, $m<n$ :\n$$\nG_{l \\times m}=\\left(\\begin{array}{ccc}\ng_1\\left(x_1\\right) & \\cdots & g_m\\left(x_1\\right) \\\\\n\\ldots & \\cdots & \\ldots \\\\\ng_1\\left(x_l\\right) & \\cdots & g_m\\left(x_l\\right)\n\\end{array}\\right)=\\left(\\begin{array}{c}\nz_1 \\\\\n\\ldots \\\\\nz_l\n\\end{array}\\right) .\n$$\nПотребуем, чтобы исходные признаковые описания можно было восстановить по новым описаниям с помощью некоторого линейного преобразования, определяемого матрицей $U=\\left(u_{j s}\\right)_{n \\times m}$ :\n$$\n\\hat{f}_j(x)=\\sum_{s=1}^m g_s(x) u_{j s}, j=1, \\ldots, n, x \\in X\n$$\n\nили в векторной записи: $\\hat{x}=z U^T$. Восстановленное описание $\\hat{x}$ не обязано в точности совпадать с исходным описанием $x$, но их отличие на объектах обучающей выборки должно быть как можно меньше при выбранной размерности $m$. Будем искать одновременно и матрицу новых признаковых описаний $G$, и матрицу линейного преобразования $U$, при которых суммарная невязка $\\Delta^2(G, U)$ восстановленных описаний минимальна:\n$$\n\\Delta^2(G, U)=\\sum_{i=1}^l\\left\\|\\hat{x}_i-x_i\\right\\|^2=\\sum_{i=1}^l\\left\\|z_i U^T-x_i\\right\\|^2=\\left\\|G U^T-F\\right\\|^2 \\rightarrow \\min _{G, U},\n$$\nгде все нормы евклидовы.\nБудем предполагать, что матрицы $G$ и $U$ невырождены: $\\operatorname{rank} G=\\operatorname{rank} U=m$. Иначе существовало бы представление $\\bar{G} \\bar{U}^T=G U^T$ с числом столбцов в матрице $\\bar{G}$, меньшим $m$. Поэтому интересны лишь случаи, когда $m \\leq \\operatorname{rank} F$.\n\nИсчерпывающее решение сформулированной задачи даёт следующая теорема.\nТеорема:\n\nЕсли $m \\leq \\operatorname{rank}  F$, то минимум $\\Delta^2(G, U)$ достигается, когда столбцы матрицы $U$ есть собственные векторы $F^T F$, соответствующие $m$ максимальным собственным значениям. При этом $G=F U$, матрицы $U$ и $G$ ортогональны.\n\nДоказательство:\n\nЗапишем необходимые условия минимума:\n$$\n\\begin{array}{l}\n\\frac{\\partial \\Delta^2}{\\partial G}=(G U^T-F) U=0 ; \n\\frac{\\partial \\Delta^2}{\\partial U}=G^T(G U^T-F)=0 .\n\\end{array}.\n$$\nПоскольку искомые матрицы $G$ и $U$ невырождены, отсюда следует:\n$$\n\\begin{array}{l}\n G=F U(U^T U)^{-1} ; \n U=F^T G(G^T G)^{-1} \n\\end{array}\n$$\nФункционал $\\Delta^2(G, U)$ зависит только от произведения матриц $G U^T$, поэтому решение задачи $\\Delta^2(G, U) \\rightarrow \\min _{G, U}$ определено с точностью до произвольного невырожденного преобразования $R: G U^T=(G R)(R^{-1} U^T)$. Распорядимся свободой выбора $R$ так, чтобы матрицы $U^T U$ и $G^T G$ оказались диагональными. Покажем, что это всегда возможно.\nПусть $\\tilde{G} \\tilde{U}^T$ - произвольное решение задачи.\nМатрица $\\tilde{U}^T \\tilde{U}$ симметричная, невырожденная, положительно определенная, поэтому существует невырожденная матрица $S_{m \\times m}$ такая, что $S^{-1} \\tilde{U}^T \\tilde{U}(S^{-1})^T=I_m$.\nМатрица $S^T \\tilde{G}^T \\tilde{G} S$ симметричная и невырожденная, поэтому существует ортогональная матрица $T_{m \\times m}$ такая, что $T^T(S^T \\tilde{G}^T \\tilde{G} S) T=d i a g(\\lambda_1, \\ldots, \\lambda_m) \\equiv \\Lambda-$ диагональная матрица. По определению ортогональности $T^T T=I_m$.\nПреобразование $R=S T$ невырождено. Положим $G=\\tilde{G} R, U^T=R^{-1} \\tilde{U}^T$. Тогда\n$$\n\\begin{aligned}\n& G^T G=T^T(S^T \\tilde{G}^T \\tilde{G} S) T=\\Lambda ; \\\\\n& U^T U=T^{-1}(S^{-1} \\tilde{U}^T \\tilde{U}(S^{-1})^T)(T^{-1})^T=(T^T T)^{-1}=I_m .\n\\end{aligned}\n$$\nВ силу $G U^T=\\tilde{G} \\tilde{U}^T$ матрицы $G$ и $U$ являются решением задачи $\\Delta^2(G, U) \\rightarrow \\min _{G, U}$ и удовлетворяют необходимому условию минимума. Подставим матрицы $G$ и $U$ в\n$$\n\\begin{aligned}\n& G=F U(U^T U)^{-1} ; \\\\\n& U=F^T G(G^T G)^{-1} .\n\\end{aligned}\n$$\nБлагодаря диагональности $G^T G$ и $U^T U$ соотношения существенно упростятся:\n\n\n\n\n\n\n## Источники\n[1]  Лекция \"Регрессионный анализ и метод главных компонентов\" — К.В. Воронцов, курс \"Машинное обучение\" 2014\n\n\n\n\n\n# WordTour \nВ этом разделе мы представляем предлагаемый нами метод WORDTOUR. В идеале мы хотели бы сохранить всю семантику в наших одномерных вложениях. Однако такие идеальные вложения вряд ли существуют, потому что отношения между словами по своей природе многомерны. Действительно, хотя Рисунок 1: Иллюстрация WORDTOUR. Каждая точка представляет слово с его координатами в качестве вектора встраивания. существующие исследования пытались уменьшить размерность вложений слов, они требуют как минимум десятков измерений (Raunak et al., 2019; Acharya et al., 2019) и нескольких измерений даже в неевклидовых пространствах (Nickel and Kiela, 2017; Тифри и др., 2019). Эти результаты показывают, что идеальных одномерных вложений не существует. Поэтому мы идем на компромисс. Мы разбиваем требования к вложениям слов на следующие две категории: Правильность Близкие вложения должны иметь семантически сходные значения. Полнота. Семантически близкие слова должны быть вставлены близко друг к другу. В WORDTOUR мы отказываемся от последнего состояния и сосредотачиваемся на первом состоянии. Например, две красные звезды на рис. 1 далеки по порядку, хотя семантически похожи. WORDTOUR принимает такое несоответствие. Из-за неполноты WORDTOUR может дать сбой в некоторых приложениях встраивания слов, таких как аналогия слов и извлечение отношений. Тем не менее, WORDTOUR по-прежнему имеет некоторые другие приложения, такие как замена слов и поиск документов. Действительно, WORDTOUR может пропустить некоторые релевантные документы, потому что они могут содержать релевантные слова далеко друг от друга. Однако близкие документы, найденные WORDTOUR, действительно близки в силу достоверности. Эти идеи показывают, что существуют одномерные вложения, которые полезны для некоторых, если не для всех, приложений. Естественным критерием правильности является то, что слова, идущие подряд в порядке, должны быть близки друг к другу в исходном пространстве вложения. Сформулируем задачу следующим образом: минимизировать σ∈P([n]) \n∥xσ1 − xσn ∥ + n−1 X i=1 ∥xσi −xσi+1 ∥.\nМы обрабатываем порядок выполнения цикла, а не пути, добавляя термин∥xσ1 −xσn ∥.За этим дизайном стоит идея о том, что мы хотели бы использовать все слова симметрично и хотели бы, чтобы граничные слова имели одинаковое количество соседних, а не граничных слов. Информирование(1), мы выбираем Tel 2norm для простоты.Однако наша формулировка является диагностической для функции сопротивления.Когда корпорация работает, мы можем подсчитать количество совпадений, т.е.P i#совпадений(σi,σi+1), как функцию стоимости.Мы продолжаем изучать другие модели в качестве будущей работы и фокусируемся на том, что касается этой статьи. Проблема оптимизации(1) является примером проблемы управления путешествиями (TSP), которая является сложной.Поскольку размер проблемы относительно велик в нашем случае, например, n=40000, это может показаться невозможным для решения проблемы.Однако на практике были разработаны высокоэффективные решения. Среди прочего, мы используем решение Хелкхауна (Helsgaun, 2018), которое реализует алгоритм Хелкернигана (LinandKernighan, 1973; Helsgaun, 2000) в высокоэффективном и действенном способе. Решатель выполняет ограниченный локальный поиск, основанный на графическом интерфейсе, сконструированном с использованием конкретной проблемы.Хельсгаун(2018) сообщил, что решение Elkh фактически решило проблему в 109399 городах Германии.Inaddition,severaleffectivealgorithmsforcomputinglowerboundsprovidetheoreticalguaranteesforthequalityofasolution.Мы используем одно-древовидную нижнюю границу (Хельсгаун, 2000), реализованную в ELKHOLVERT, для вычисления нижних границ оптимального значения.Поскольку туризм является особым случаем одного дерева, минимальное количество Стоунетри является приемлемым для решения этой проблемы. Алгоритм поиска потенциального вектора для узкоспециализированного градиента зарождается.WORDTOUR вычисляет минимально оптимальное решение проблемы(1) с помощью решения и использует это решение в порядке слов, т.е. словосочетания.\n ",
      "metadata": {
        "tags": [],
        "cell_id": "58bd8b80ae044acc9ffc9427fbb5919c",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "58bd8b80ae044acc9ffc9427fbb5919c"
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "4d2a6db5fb6c42a49db54022c054523b",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "4d2a6db5fb6c42a49db54022c054523b"
    },
    {
      "cell_type": "markdown",
      "source": "# Нормальный текст \n\n## PCA\nВ качестве сравнения, рассмотрим один из методов миниманизации размерности данных - PCA (principal component analysis).\nСуть данного метода заключается в сведении к случаю пространства $\\mathbb {R}^{1}$\nПусть дана матрица размерностью $n \\times d $  нужно привести её к виду $n \\times 1$,т.е нам нужно рассмотреть матрицу состояний F в пространстве $\\mathbb {R}^{n}$ :\n$$\nF_{l \\times n}=\\left(\\begin{array}{ccc}\nf_1\\left(x_1\\right) & \\ldots & f_n\\left(x_1\\right) \\\\\n\\ldots & \\ldots & \\ldots \\\\\nf_1\\left(x_l\\right) & \\ldots & f_n\\left(x_l\\right)\n\\end{array}\\right)=\\left(\\begin{array}{c}\nx_1 \\\\\n\\ldots \\\\\nx_l\n\\end{array}\\right) .\n$$\nЧерез $z_i=\\left(g_1\\left(x_i\\right), \\ldots, g_m\\left(x_i\\right)\\right)$ запишем описания объектов, но уже в новом пространстве $Z=\\mathbb{R}^m$ с размерностью меньшей $n$, $m<n$ :\n$$\nG_{l \\times m}=\\left(\\begin{array}{ccc}\ng_1\\left(x_1\\right) & \\cdots & g_m\\left(x_1\\right) \\\\\n\\ldots & \\cdots & \\ldots \\\\\ng_1\\left(x_l\\right) & \\cdots & g_m\\left(x_l\\right)\n\\end{array}\\right)=\\left(\\begin{array}{c}\nz_1 \\\\\n\\ldots \\\\\nz_l\n\\end{array}\\right) .\n$$\nТак же поставим условие, что с помощью матрицы $U=\\left(u_{j s}\\right)_{n \\times m}$ можно перейти обратно из m-мерного в пространство размерностью n:\n$$\n\\hat{f}_j(x)=\\sum_{s=1}^m g_s(x) u_{j s}, j=1, \\ldots, n, x \\in X\n$$\n\nиначе в векторной форме: $\\hat{x}=z U^T$. Восстановленное описание $\\hat{x}$ может в точности не совпадать с первоначальным $x$, но их отличие на объектах обучающей выборки должно быть как можно меньше при выбранной размерности $m$. Будем искать одновременно и матрицу новых признаковых описаний $G$, и матрицу линейного преобразования $U$, при которых суммарная невязка $\\Delta^2(G, U)$ восстановленных описаний минимальна:\n$$\n\\Delta^2(G, U)=\\sum_{i=1}^l\\left\\|\\hat{x}_i-x_i\\right\\|^2=\\sum_{i=1}^l\\left\\|z_i U^T-x_i\\right\\|^2=\\left\\|G U^T-F\\right\\|^2 \\rightarrow \\min _{G, U},\n$$\nгде все нормы евклидовы.\n \nРешение явно получается с помощью слудующей теоремы:\nЕсли $m \\leq \\operatorname{rank}  F$, то минимум $\\Delta^2(G, U)$ достигается, когда столбцы матрицы $U$ есть собственные векторы $F^T F$, соответствующие $m$ максимальным собственным значениям. При этом $G=F U$, матрицы $U$ и $G$ ортогональны.\n\n\n# WordTour\nОсновой метода WordTour является  разбиение требований к вложениям слов на следующие пункты: \n 1. Правильность. Имеет место семантически сходные значения для близких вложений.  \n 2. Полнота. Семантически близкие слова должны быть вставлены близко друг к другу.     \nДанный метод игнорирует пункт 2 и полностью сосредотачивается на пункте 1. Например, две звезды помеченные красным цветом на рис. 1 удалены по порядку, но семантически близки. WordTour пропускает данное несоответствие. Из-за неполноты метод может дать сбой в некоторых местах встраивания слов, таких как аналогия слов и извлечение отношений. Действительно, WordTour может пропустить некоторые релевантные документы, потому что они могут содержать релевантные слова далеко друг от друга. Однако близкие слова, найденные данным методом , действительно близки в силу достоверности. Эти идеи показывают, что существуют одномерные вложения, которые полезны для некоторых, если не для всех, приложений. Естественным критерием правильности является то, что слова, идущие подряд в порядке, должны быть близки друг к другу в исходном пространстве вложения. Тогда математически задача формируется следующим образом: минимизировать σ∈P([n]) \n$$\n\\min _{\\sigma \\in \\mathcal{P}([n])}\\left\\|\\boldsymbol{x}_{\\sigma_1}-\\boldsymbol{x}_{\\sigma_n}\\right\\|+\\sum_{i=1}^{n-1}\\left\\|\\boldsymbol{x}_{\\sigma_i}-\\boldsymbol{x}_{\\sigma_{i+1}}\\right\\| (1)\n$$\n\nМетодом обрабатываются не пути, а порядок выполнения цикла, в терминах $\\left\\|\\boldsymbol{x}_{\\sigma_1}-\\boldsymbol{x}_{\\sigma_n}\\right\\|$. За этой формулировкой стоит идея о том, что требуется использовать все слова симметрично и, возможно , чтобы граничные слова имели одинаковое количество соседних, а не граничных слов. \nВ формуле (1) используется метрика пространства $L_2$ для упрощения вычеслений.Однако данную формулу можно рассматривать, как диагностическую для функции сопротивления.Что дает возможность подсчитать количество совпадений, $\\sum_1 \\#$ co-occurrences of $\\left(\\sigma_i, \\sigma_{i+1}\\right)$, как функцию стоимости.\n![Рисунок 1](image-20230213-220822.png)\n\n",
      "metadata": {
        "tags": [],
        "cell_id": "697d0b39e415467aa03566bef0434145",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "697d0b39e415467aa03566bef0434145"
    },
    {
      "cell_type": "markdown",
      "source": "# Источники\n\n1 Источник (PCA): \n@article{article,\nauthor = {Greenacre, Michael and Groenen, Patrick and Hastie, Trevor and Iodice D'Enza, Alfonso and Markos, Angelos and Tuzhilina, Elena},\nyear = {2022},\nmonth = {12},\npages = {100},\ntitle = {Principal component analysis},\nvolume = {2},\ndoi = {10.1038/s43586-022-00184-w}\n}\n\n2 Источник(WordTour):\n@inproceedings{sato2022wordtour,\n  author    = {Ryoma Sato},\n  title     = {Word Tour: One-dimensional Word Embeddings via the Traveling Salesman Problem},\n  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT}},\n  year      = {2022},\n}",
      "metadata": {
        "tags": [],
        "cell_id": "f17e3c76c7374e9f87cccc46b61bc44f",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "f17e3c76c7374e9f87cccc46b61bc44f"
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0d79deab-c9c6-414b-8803-1e385542ff1f' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {},
    "orig_nbformat": 2,
    "deepnote_notebook_id": "fcdbf9552ef646acb38f0aefad0f5053",
    "deepnote_execution_queue": []
  }
}